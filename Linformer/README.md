# Linformer: Self-Attention with Linear Complexity

This a pytorch implementation of the Linformer model, which reduces transformer Self-Attention Complexity from O(n^2) 
to O(n). This implementation is based on this paper by Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma.
https://arxiv.org/abs/2006.04768
